
> **T5**\
> **Report**

**Template**
+-----------------------------------+-----------------------------------+
| **Field**                         | **Description**                   |
+===================================+===================================+
| **Title**                         | > The title of the AI Bootcamp    |
|                                   | > Project that summarize the main |
|                                   | > focus and objective of the      |
|                                   | > project.                        |
+-----------------------------------+-----------------------------------+
| **Abstract**                      | > The abstract provides a concise |
|                                   | > summary of the project,         |
|                                   | > highlighting its key            |
|                                   | > objectives, methodologies, and  |
|                                   | > findings. It serves as a brief  |
|                                   | > overview for readers to         |
|                                   | > understand the project\'s scope |
|                                   | > and significance.               |
+-----------------------------------+-----------------------------------+
| **Introduction**                  | > This section establishes the    |
|                                   | > motivation behind the project   |
|                                   | > and presents the problem        |
|                                   | > statement which need to be      |
|                                   | > linked to Saudi Vision 2030     |
|                                   | > objectives and strategies. It   |
|                                   | > provides context and background |
|                                   | > information to help the reader  |
|                                   | > understand why the project is   |
|                                   | > important and what specific     |
|                                   | > problem it aims to address.     |
+-----------------------------------+-----------------------------------+
| > **Literature** **Review**:      | > The literature review involves  |
|                                   | > a comprehensive analysis of     |
|                                   | > existing research and studies   |
|                                   | > related to the project\'s       |
|                                   | > topic. It examines the current  |
|                                   | > state of knowledge, identifies  |
|                                   | > gaps or limitations in previous |
|                                   | > work, and highlights relevant   |
|                                   | > theories, methodologies, or     |
|                                   | > frameworks that inform the      |
|                                   | > project\'s approach.            |
+-----------------------------------+-----------------------------------+
| > **Data Description** **and      | > This section provides a         |
| > Structure** :                   | > detailed description of the     |
|                                   | > data used in the project. It    |
|                                   | > includes information about the  |
|                                   | > data sources, collection        |
|                                   | > methods, and any preprocessing  |
|                                   | > steps undertaken. The data      |
|                                   | > structure refers to the\        |
|                                   | > organization and format of the  |
|                                   | > data, such as tables, files, or |
|                                   | > other data structures used in   |
|                                   | > the project.                    |
+-----------------------------------+-----------------------------------+
| **Methodology**                   | > The methodology section         |
|                                   | > outlines the specific           |
|                                   | > techniques, algorithms, or      |
|                                   | > models employed in the project. |
|                                   | > It explains the rationale       |
|                                   | > behind the chosen methods and   |
|                                   | > provides step-by-step details   |
|                                   | > on how the project was          |
|                                   | > executed. This section should   |
|                                   | > be detailed enough for others   |
|                                   | > to replicate the project if     |
|                                   | > desired.                        |
+-----------------------------------+-----------------------------------+
| > **Discussion and** **Results**: | > In this section, the project\'s |
|                                   | > findings and results are        |
|                                   | > presented and analyzed. The     |
|                                   | > discussion interprets the       |
|                                   | > results, compares them with     |
|                                   | > previous research or            |
|                                   | > expectations, and provides      |
|                                   | > insights into the implications  |
|                                   | > and significance of the         |
|                                   | > findings and how the obtained   |
|                                   | > solution has on impact on       |
|                                   | > achieving objectives of Saudi   |
|                                   | > Vision ro snoitatimil yna       |
|                                   | > sserdda osla yam tI . 2030 .    |
|                                   | > tcejorp eht gnirud deretnuocne  |
|                                   | > segnellahc                      |
+-----------------------------------+-----------------------------------+
| > **Conclusion and** **Future     | > The conclusion summarizes the   |
| > Work**                          | > main findings of the project    |
|                                   | > and restates its significance.  |
|                                   | > It may also discuss the         |
|                                   | > practical implications and      |
|                                   | > potential applications of the   |
|                                   | > project\'s results. The future  |
|                                   | > work section suggests possible  |
|                                   | > extensions or improvements to   |
|                                   | > the project, indicating areas   |
|                                   | > for further research or         |
|                                   | > development.                    |
+-----------------------------------+-----------------------------------+
| **Team**                          |                                   |
+-----------------------------------+-----------------------------------+

2

![](vertopal_4083851ead46433c99b30fbd81535567/media/image2.png){width="9.936111111111112in"
height="0.8222222222222222in"}![](vertopal_4083851ead46433c99b30fbd81535567/media/image3.png){width="11.25in"
height="8.4375in"}![](vertopal_4083851ead46433c99b30fbd81535567/media/image4.png){width="7.358332239720035in"
height="1.2489391951006124in"}

+-----------------------------------------------------------------------+
| > **Saudi Solar Vision:** Shining a Light on Tomorrow                 |
+=======================================================================+
+-----------------------------------------------------------------------+

3

![](vertopal_4083851ead46433c99b30fbd81535567/media/image2.png){width="9.936111111111112in"
height="0.8222222222222222in"}![](vertopal_4083851ead46433c99b30fbd81535567/media/image3.png){width="11.25in"
height="8.4375in"}

> **Abstract**
>
> Dirt on solar panels can significantly reduce their energy output,
> making it essential to detect and classify dirt\'s presence promptly.
> Given the increasing use of solar energy systems for power generation,
> developing accurate and efficient methods for detecting and
> classifying the presence of dirt on solar panels is becoming
> increasingly important. Therefore, this project aims to utilize Deep
> Learning (DL) models for classifying the presence of dirt on solar
> panels using a dataset of 940 images, comprising of 139 Electrical
> Damage, 157 Physical Damage, 244 Weather Damage, 297 Clean, and 103
> Snow Covered images. According to the results, MobileNet outperformed
> other models with an overall accuracy of 0.91, precision of 0.92,
> recall of 0.91, and loss value of 0.39. The results of this project
> provide important insights into the development of accurate and
> efficient methods for detecting and classifying the dirt on solar
> panels. By leveraging DL techniques, highly accurate models can be
> developed to detect dirt on solar panels, which is critical for
> maintaining the optimal performance and efficiency of solar energy
> systems. Hence, the proposed model provides a valuable resource for
> researchers and practitioners interested in developing accurate and
> efficient methods for detecting and classifying the dirt on solar
> panels. Furthermore, we have done s prediction model for predicting
> the generated energy power from the solar panels, along with Saudi
> Solar Vision chatbot, that designed to answer all questions related to
> all Saudi project related to renewable energy fields. Also, a
> recommendation system was designed to suggest similar location to the
> location that has been given.

4

![](vertopal_4083851ead46433c99b30fbd81535567/media/image2.png){width="9.936111111111112in"
height="0.8222222222222222in"}![](vertopal_4083851ead46433c99b30fbd81535567/media/image3.png){width="11.25in"
height="8.4375in"}

> **Introduction**
>
> Solar energy is a sustainable and renewable source has recently

+--------+--------+--------+--------+--------+--------+--------+--------+
| gained | signi  | atte   | Solar  | p      | also   | known  | > as   |
|        | ficant | ntion. |        | anels, |        |        |        |
+========+========+========+========+========+========+========+========+
+--------+--------+--------+--------+--------+--------+--------+--------+

> photovoltaic panels, harvest solar energy from the sun to provide the
> energy we use every day. Using renewable energy is commonly viewed as
> contingent upon developing sustainable energy sources, decreasing
> reliance on fossil fuels, and mitigating climate change. It is
> essential for halting global warming and cutting greenhouse gas
> emissions. Additionally, it might minimize the amount of water needed
> to produce energy and enhance air quality. However, solar panel
> installation on land may adversely affect nearby species, habitats,
> soil, and water supplies. Using photovoltaic cells, solar panels
> generate electricity by converting solar energy. Electrons move when
> sunlight strikes a solar panel\'s surface, generating an electrical
> current. The semiconductor materials used in the photovoltaic cells in
> the solar panel, such as silicon, can capture solar energy and
> transform it into electrical power. After being generated by the solar
> panels, the power is conveyed to an inverter that transforms the
> Direct Current (DC) electricity produced by the solar panels into
> Alternating Current (AC) electricity, which can be used to power
> buildings. Solar panels can be put on rooftops or in enormous solar
> farms to provide sustainable energy and lessen reliance on fossil
> fuels. These panels can either expand the electrical supply of a
> building or provide electricity in remote or off-grid locations.

5

![](vertopal_4083851ead46433c99b30fbd81535567/media/image2.png){width="9.936111111111112in"
height="0.8222222222222222in"}![](vertopal_4083851ead46433c99b30fbd81535567/media/image3.png){width="11.25in"
height="8.4375in"}

> **Data Description and Structure** :

+-----------------------------------+-----------------------------------+
| \-                                | > Image Dataset:                  |
+===================================+===================================+
| > The effective maintenance of    |                                   |
| > clean solar panels is vital for |                                   |
| >                                 |                                   |
| > maximizing the efficiency and   |                                   |
| > output of solar energy systems. |                                   |
| >                                 |                                   |
| > The accumulation of dirt on     |                                   |
| > solar panels can significantly  |                                   |
| >                                 |                                   |
| > decrease their energy           |                                   |
| > production, underscoring the    |                                   |
| >                                 |                                   |
| > importance of timely detection  |                                   |
| > and mitigation of dirt          |                                   |
| > presence.                       |                                   |
| >                                 |                                   |
| > In this project, the dataset    |                                   |
| > for solar panels was collected  |                                   |
| > from                            |                                   |
| >                                 |                                   |
| > three different resources:      |                                   |
| >                                 |                                   |
| > \- Solar Panel Images (Clean    |                                   |
| > and Faulty Images).             |                                   |
| >                                 |                                   |
| > \- Solar Panel dust images.     |                                   |
| >                                 |                                   |
| > \- Manually collected dataset.  |                                   |
| >                                 |                                   |
| > Table 1 shows the details for   |                                   |
| > each dataset.                   |                                   |
+-----------------------------------+-----------------------------------+

*Table 1 Solar Panels Image Dataset Details*

+-----------------------+-----------------------+-----------------------+
| > **Solar Panels      | **Solar Panels dust   | **Manually collected  |
| > Faulty and Clean**  | images**              | dataset**             |
| > **Images**          |                       |                       |
+=======================+=======================+=======================+
| > This dataset has 6  | > This dataset has 2  | > This dataset was    |
| > different           | > classes: Clean (    | > collected           |
| >                     | > 1493 images)\       | >                     |
| > classes:            | > Dusty ( 1069        | > manually, and it    |
| >                     | > images)             | > has 2 classes:      |
| > Bird Drop ( 207     |                       | >                     |
| > images)             |                       | > Leaves ( 19 images) |
| >                     |                       | >                     |
| > Clean ( 193 images) |                       | > Rain ( 63 images)   |
| >                     |                       |                       |
| > Dusty ( 190 images) |                       |                       |
| >                     |                       |                       |
| > Electrical Damage ( |                       |                       |
| > 103                 |                       |                       |
| >                     |                       |                       |
| > images)             |                       |                       |
| >                     |                       |                       |
| > Physical Damage (   |                       |                       |
| > 69 images)          |                       |                       |
| >                     |                       |                       |
| > Snow Covered ( 123  |                       |                       |
| > images)             |                       |                       |
+-----------------------+-----------------------+-----------------------+

> In the pre-processing steps for the image data, we merged the three
> datasets into a single dataset. We combined the Physical Damage and
> Electrical Damage from the first dataset into a single file called
> \"Electrical Damage\", since both files have the same effect on solar
> panels, which would produce zero energy. Furthermore, as Leaves and
> Bird Drops are two physical elements that have the potential to lessen
> the energy generated by solar panels, we\'ve joined them into a single
> file named \"Physical Damage\" from the first dataset and the manually
> collected dataset. Moreover, we have created a file named \"Clean\" by
> combining the two Clean files from the first and second datasets. As
> both dust and rain are caused by the weather, we subsequently merged
> the Dusty file from the first dataset and the Rain file from the
> manually collected dataset into a single file called \"Weather
> Damage.\" The final file is called \"Snow Covered,\" and as it\'s a
> seasonal damage, we use it exactly as is. The processing of the
> picture collection is shown in Figure 1. Every image that was
> redundant or unreliable was eliminated.

6

![](vertopal_4083851ead46433c99b30fbd81535567/media/image2.png){width="9.936111111111112in"
height="0.8222222222222222in"}![](vertopal_4083851ead46433c99b30fbd81535567/media/image5.png){width="11.25in"
height="16.489583333333332in"}

> **Data Description and Structure** :
>
> *Figure 1 Solar Panels Image Dataset Preprocessing*
>
> The final dataset consists of 940 images, 297 images were classified
> as Clean, 139 images were Electrical Damage, 157 images were Physical
> Damage, 244 images were Weather Damage, and 103 images were Snow
> Covered.

7

![](vertopal_4083851ead46433c99b30fbd81535567/media/image2.png){width="9.936111111111112in"
height="0.8222222222222222in"}![](vertopal_4083851ead46433c99b30fbd81535567/media/image3.png){width="11.25in"
height="8.4375in"}

> **Data Description and Structure** :

+-----------------------------------+-----------------------------------+
| \-                                | > Image Preprocessing:            |
+===================================+===================================+
| > Pre-processing plays an         |                                   |
| > important role in DL to enhance |                                   |
| > the quality and suitability of  |                                   |
| > images for effective analysis.  |                                   |
| > Applying pre-processing         |                                   |
| > techniques helps reduce         |                                   |
| > unwanted distortions while      |                                   |
| > highlighting the important      |                                   |
| > characteristics relevant to the |                                   |
| > specific application. In the    |                                   |
| > context of image analysis,      |                                   |
| > pre-processing typically begins |                                   |
| > with data augmentation, a       |                                   |
| > process facilitated by the use  |                                   |
| > of the 'ImageDataGenerator'     |                                   |
| > module from the Keras library   |                                   |
| > in Python. This module provides |                                   |
| > for the development of          |                                   |
| > augmented pictures within the   |                                   |
| > dataset, allowing for the       |                                   |
| > insertion of variants and the   |                                   |
| > diversification of the training |                                   |
| > data. Through pre-processing,   |                                   |
| > the images are prepared to meet |                                   |
| > the requirements of DL models,  |                                   |
| > ultimately leading to more      |                                   |
| > accurate and robust analysis    |                                   |
| > outcomes.                       |                                   |
+-----------------------------------+-----------------------------------+

8

![](vertopal_4083851ead46433c99b30fbd81535567/media/image2.png){width="9.936111111111112in"
height="0.8222222222222222in"}![](vertopal_4083851ead46433c99b30fbd81535567/media/image3.png){width="11.25in"
height="8.4375in"}

> **Data Description and Structure** :

+-----------------------------------+-----------------------------------+
| •                                 | > Classification:                 |
+===================================+===================================+
| > In the pre-processing stage,    |                                   |
| > the dataset underwent shuffling |                                   |
| > to                              |                                   |
| >                                 |                                   |
| > ensure randomization. After     |                                   |
| > shuffling, the dataset was      |                                   |
| > divided into                    |                                   |
| >                                 |                                   |
| > three sets: the training set,   |                                   |
| > validation set, and test set.   |                                   |
| > The                             |                                   |
| >                                 |                                   |
| > proportions of these sets were  |                                   |
| > allocated as 80%, 10%, and 10%  |                                   |
| > of                              |                                   |
| >                                 |                                   |
| > the total dataset,              |                                   |
| > respectively. The augmentation  |                                   |
| > techniques were                 |                                   |
| >                                 |                                   |
| > applied to the training dataset |                                   |
| > to enhance its diversity and    |                                   |
| > improve                         |                                   |
| >                                 |                                   |
| > the model\'s generalization     |                                   |
| > capabilities. Firstly, the      |                                   |
| > pixel values of                 |                                   |
| >                                 |                                   |
| > the images were rescaled using  |                                   |
| > a factor of 1.0/255, converting |                                   |
| > the                             |                                   |
| >                                 |                                   |
| > pixel range from \[0, 255\] to  |                                   |
| > \[0, 1\]. This normalization    |                                   |
| > ensures that                    |                                   |
| >                                 |                                   |
| > all features have a consistent  |                                   |
| > scale and facilitates the       |                                   |
| > learning                        |                                   |
| >                                 |                                   |
| > process. Additionally,          |                                   |
| > augmentation was performed by   |                                   |
| > applying                        |                                   |
| >                                 |                                   |
| > various transformations. The    |                                   |
| > 'shear_range' parameter was set |                                   |
| > to                              |                                   |
| >                                 |                                   |
| > 0.3, a range of -0.3 to 0.3 was |                                   |
| > randomly applied to the images, |                                   |
| >                                 |                                   |
| > introducing deformations that   |                                   |
| > simulate tilting or skewing.    |                                   |
| > Moreover,                       |                                   |
| >                                 |                                   |
| > the images were magnified by a  |                                   |
| > factor of 0.5 using the         |                                   |
| >                                 |                                   |
| > 'zoom_range' parameter,         |                                   |
| > enlarging specific regions of   |                                   |
| > the images.                     |                                   |
| >                                 |                                   |
| > Horizontal flipping, vertical   |                                   |
| > flipping, and random rotations  |                                   |
| > within the                      |                                   |
| >                                 |                                   |
| > range of 0 to 45 degrees were   |                                   |
| > also applied. Furthermore, a    |                                   |
| >                                 |                                   |
| > maximum horizontal and vertical |                                   |
| > shifts or translations of 30%   |                                   |
| > of the                          |                                   |
| >                                 |                                   |
| > image\'s total width and height |                                   |
| > were allowed using the          |                                   |
| >                                 |                                   |
| > 'width_shift_range' and         |                                   |
| > 'height_shift_range'            |                                   |
| > parameters. Brightness          |                                   |
| >                                 |                                   |
| > adjustment was performed using  |                                   |
| > the 'brightness_range'          |                                   |
| > parameter,                      |                                   |
| >                                 |                                   |
| > setting the brightness range    |                                   |
| > from 0.2 to 1.2. For the        |                                   |
| > validation and                  |                                   |
| >                                 |                                   |
| > test datasets, only pixel       |                                   |
| > rescaling by the factor of      |                                   |
| > 1.0/255 was                     |                                   |
| >                                 |                                   |
| > applied. The images in the      |                                   |
| > training, validation, and       |                                   |
| > testing datasets                |                                   |
| >                                 |                                   |
| > were uniformly resized to       |                                   |
| > 224x224 pixels as the target    |                                   |
| > size.                           |                                   |
| >                                 |                                   |
| > Additionally, a batch size of   |                                   |
| > 32 was assigned to each         |                                   |
| > dataset. The                    |                                   |
| >                                 |                                   |
| > class mode was set to           |                                   |
| > 'categorical' to indicate a     |                                   |
| > multi-class                     |                                   |
| >                                 |                                   |
| > classification task. The color  |                                   |
| > mode was set to RGB,            |                                   |
| > representing                    |                                   |
| >                                 |                                   |
| > the red, green, and blue color  |                                   |
| > channels of the images. Lastly, |                                   |
| > the                             |                                   |
| >                                 |                                   |
| > seed and random state values of |                                   |
| > 42 were sets to ensure          |                                   |
| >                                 |                                   |
| > reproducibility of the data     |                                   |
| > generation process. These       |                                   |
| > preprocessing                   |                                   |
| >                                 |                                   |
| > steps enable the DL model to    |                                   |
| > learn effectively from the      |                                   |
| > augmented                       |                                   |
| >                                 |                                   |
| > training data and produce more  |                                   |
| > robust and accurate             |                                   |
| > predictions.                    |                                   |
+-----------------------------------+-----------------------------------+

9

![](vertopal_4083851ead46433c99b30fbd81535567/media/image2.png){width="9.936111111111112in"
height="0.8222222222222222in"}![](vertopal_4083851ead46433c99b30fbd81535567/media/image3.png){width="11.25in"
height="8.4375in"}

> **Data Description and Structure** :

+-----------------------------------------------------------------------+
| > • Object Detection:                                                 |
| >                                                                     |
| > Image annotation preprocessing is a crucial step in enhancing the   |
| >                                                                     |
| > diversity and quality of datasets for machine learning              |
| > applications,                                                       |
| >                                                                     |
| > particularly in the context of computer vision. The provided data   |
| >                                                                     |
| > outlines a comprehensive set of augmentation techniques applied     |
| >                                                                     |
| > during the annotation process. The annotation of the images was     |
| >                                                                     |
| > done manually by using RoboFlow. After that, the augmentation       |
| >                                                                     |
| > techniques were applied. Horizontal and vertical flips, along with  |
| >                                                                     |
| > 90° rotations in both clockwise and counter-clockwise directions,   |
| >                                                                     |
| > contribute to creating variations in object orientations.           |
| > Additionally,                                                       |
| >                                                                     |
| > upside-down flips introduce further complexity to the dataset.      |
| >                                                                     |
| > Cropping, ranging from 0% minimum zoom to 37% maximum zoom,         |
| >                                                                     |
| > ensures that the model encounters objects at different scales,      |
| >                                                                     |
| > enriching its understanding of varying perspectives. The            |
| > application                                                         |
| >                                                                     |
| > of noise, affecting up to 9% of pixels, simulates real-world        |
| >                                                                     |
| > imperfections and enhances the model\'s robustness. The             |
| >                                                                     |
| > introduction of cutout, involving the removal of portions in 5      |
| > boxes,                                                              |
| >                                                                     |
| > each covering 12% of the image, serves to encourage the model to    |
| >                                                                     |
| > focus on relevant features while fostering generalization.          |
| > Moreover,                                                           |
| >                                                                     |
| > preprocessing techniques include auto-orient, applied to enhance    |
| >                                                                     |
| > the dataset\'s orientation diversity, and resizing, which stretches |
| >                                                                     |
| > images to a standardized 640x640 resolution. Collectively, these    |
| >                                                                     |
| > preprocessing techniques contribute to a more resilient and         |
| >                                                                     |
| > versatile dataset, ultimately improving the performance and         |
| >                                                                     |
| > adaptability of the models trained on annotated images, with a      |
| >                                                                     |
| > specific emphasis on object detection tasks.                        |
+=======================================================================+
+-----------------------------------------------------------------------+

10

![](vertopal_4083851ead46433c99b30fbd81535567/media/image2.png){width="9.936111111111112in"
height="0.8222222222222222in"}![](vertopal_4083851ead46433c99b30fbd81535567/media/image3.png){width="11.25in"
height="8.4375in"}

> **Data Description and Structure** :
>
> •Sensor Dataset & preprocessing:
>
> We used a tabular dataset consist of two files for 34 days that
> gathered in india and record the sensor data every 15 minutes, the
> first dataset is the generation power and consist of 7 columns which
> are DATE_TIME, PLANT_ID, SOURCE_KEY ,DC_POWER,\
> AC_POWER, DAILY_YIELD, TOTAL_YIELD, and 68778 rows. Also, the weather
> data related to the weather and sun data, consists of 6
> columns,DATE_TIME,PLANT_ID,SOURCE_KEY, AMBIENT_TEMP, MODULE_TEMP,
> IRRADIATION, and 3182 rows. In the\
> preprocessing step, we have combined the two dataset to get each
> module with its weather data, and dropped unnecessary columns
>
> **1. Location Dataset:**

+-----------------------------------+-----------------------------------+
| > •\                              | > **Dataset Overview:**\          |
| > •                               | > The dataset includes            |
|                                   | > comprehensive solar radiation   |
|                                   | > data for various locations      |
|                                   | > across Saudi Arabia.            |
+===================================+===================================+
| > •\                              | > **Key columns include**\        |
| > •                               | > Specific Photovoltaic Power     |
|                                   | > Output (PVOUT specific):This is |
|                                   | > a measure of the efficiency of  |
|                                   | > a photovoltaic (PV) system.     |
+-----------------------------------+-----------------------------------+
| > It represents the amount of     |                                   |
| > electrical energy (in           |                                   |
| > kilowatt-hours, kWh) generated  |                                   |
| > by a system per unit of peak    |                                   |
| > power capacity (kilowatt-peak,  |                                   |
| > kWp).                           |                                   |
| >                                 |                                   |
| > Direct Normal Irradiation       |                                   |
| > (DNI):DNI refers to the amount  |                                   |
| > of\                             |                                   |
| > solar radiation received per    |                                   |
| > unit area (in kilowatt-hours    |                                   |
| > per\                            |                                   |
| > square meter, kWh/m²) by a      |                                   |
| > surface that is always oriented |                                   |
| > perpendicular to the sun\'s     |                                   |
| > rays. It measures the direct    |                                   |
| > component of solar irradiance.  |                                   |
| >                                 |                                   |
| > • Global Horizontal Irradiation |                                   |
| > (GHI): GHI is the total amount  |                                   |
| > of solar radiation received per |                                   |
| > unit area (in kilowatt-hours    |                                   |
| > per square meter, kWh/m²) on a  |                                   |
| > horizontal surface. It includes |                                   |
| > both the direct sunlight and    |                                   |
| > the diffuse sky radiation.      |                                   |
| >                                 |                                   |
| > • Diffuse Horizontal            |                                   |
| > Irradiation (DIF): Diffuse      |                                   |
| > Horizontal\                     |                                   |
| > Irradiation is the component of |                                   |
| > solar radiation received from   |                                   |
| > the sky (excluding the sun\'s   |                                   |
| > direct rays) on a horizontal\   |                                   |
| > surface. It\'s measured in      |                                   |
| > kilowatt-hours per square meter |                                   |
| > (kWh/m²).                       |                                   |
+-----------------------------------+-----------------------------------+

11

![](vertopal_4083851ead46433c99b30fbd81535567/media/image2.png){width="9.936111111111112in"
height="0.8222222222222222in"}![](vertopal_4083851ead46433c99b30fbd81535567/media/image3.png){width="11.25in"
height="8.4375in"}

> **Data Description and Structure** :
>
> • Global Tilted Irradiation at Optimum Angle (GTI opta): This measures
> the total solar irradiance received per unit area (in kilowatt-hours
> per square meter, kWh/m²) on a surface that is tilted at an optimal
> angle towards the sun. The optimal angle is usually determined based
> on geographical location and time of year.
>
> • Optimum Tilt of PV Modules (OPTA):OPTA denotes the ideal angle and
> orientation for solar panels to maximize their exposure to sunlight.
> It is typically expressed in degrees, where the first number might
> indicate the tilt angle from the horizontal and the second number the
> compass orientation.
>
> • Air Temperature (TEMP):This is a straightforward measurement of the
> temperature of the air at a specific location, typically expressed in
> degrees Celsius (°C).
>
> • Terrain Elevation (ELE):Elevation refers to the height of a point on
> the Earth\'s surface above sea level, measured in meters. Terrain
> elevation can affect solar panel performance due to factors like
> atmospheric density and temperature variations with altitude.
>
> • The data was collected by scraping the data from global solar atlas•
> Dataset Characteristics:\
> • It consists of 27,689 entries, covering a broad range of latitudes
> and longitudes across the country.
>
> • The data provides a detailed view of solar potential, including
> metrics like irradiance and PV output, crucial for renewable energy
> studies.
>
> **2. Location Preprocessing**
>
> • **Data Normalization:**\
> • The dataset undergoes normalization to standardize the solar
> radiation scores and other related metrics.
>
> • This step is essential to ensure comparability and accuracy in
> subsequent analysis and modeling.
>
> • Identifying Optimal Locations:\
> • The preprocessing includes techniques to identify the best locations
> for solar energy projects, considering factors like irradiance,
> temperature, and geographical positioning.

12

![](vertopal_4083851ead46433c99b30fbd81535567/media/image2.png){width="9.936111111111112in"
height="0.8222222222222222in"}![](vertopal_4083851ead46433c99b30fbd81535567/media/image3.png){width="11.25in"
height="8.4375in"}

> **Methodology** Proposed Techniques
>
> **Image Classification**

+-----------------------------------------------------------------------+
| > Transfer Learning (TL) is a technique in DL that uses pre-trained   |
| >                                                                     |
| > models such as MobileNet, to improve the performance of new         |
| >                                                                     |
| > models for specific tasks. Utilizing the knowledge of a pre-trained |
| >                                                                     |
| > model, TL can considerably decrease the time and computational      |
| >                                                                     |
| > resources necessary for new tasks, particularly when the training   |
| >                                                                     |
| > data is restricted. Moreover, in image classification, TL involves  |
| >                                                                     |
| > fine-tuning the pre-trained model by removing the final             |
| >                                                                     |
| > classification layer and adding a new layer explicitly trained for  |
| > the                                                                 |
| >                                                                     |
| > new task.                                                           |
+=======================================================================+
+-----------------------------------------------------------------------+

> • MobileNet

+-----------------------------------------------------------------------+
| > MobileNet is a widely recognized and influential CNN                |
| >                                                                     |
| > architecture that has revolutionized the field of DL for mobile     |
| >                                                                     |
| > and embedded devices. MobileNet was proposed by Andrew              |
| >                                                                     |
| > G. Howard et al., it addresses the challenge of deploying           |
| >                                                                     |
| > powerful computer vision models on resource-constrained             |
| >                                                                     |
| > platforms. It specifically tackles the challenge of deploying       |
| >                                                                     |
| > DNN on resource-constrained devices like mobile phones and          |
| >                                                                     |
| > embedded systems. The key innovation of MobileNet lies in           |
| >                                                                     |
| > its utilization of depthwise separable convolutions, which splits   |
| >                                                                     |
| > the convolution into depthwise and pointwise convolutions.          |
| >                                                                     |
| > The former applies a single filter to each input channel            |
| >                                                                     |
| > individually to capture spatial information, while the latter       |
| >                                                                     |
| > combines the outputs of the depthwise convolution using 1x1         |
| >                                                                     |
| > convolutions to mix features across channels. By decoupling         |
| >                                                                     |
| > spatial and channel-wise filtering, MobileNet significantly         |
| >                                                                     |
| > reduces computational complexity and model size without             |
| >                                                                     |
| > sacrificing accuracy. Additionally, MobileNet introduces the        |
| >                                                                     |
| > width multiplier hyperparameter, enabling scaling of channel        |
| >                                                                     |
| > numbers in each layer to optimize the model\'s size and             |
| >                                                                     |
| > accuracy trade-off based on specific deployment                     |
| >                                                                     |
| > requirements.                                                       |
+=======================================================================+
+-----------------------------------------------------------------------+

13

![](vertopal_4083851ead46433c99b30fbd81535567/media/image2.png){width="9.936111111111112in"
height="0.8222222222222222in"}![](vertopal_4083851ead46433c99b30fbd81535567/media/image3.png){width="11.25in"
height="8.4375in"}

> **Methodology** Proposed Techniques
>
> **Object Detection**
>
> • YOLOv8

+-----------------------------------------------------------------------+
| > YOLOv8, the eighth iteration of the You Only Look Once              |
| >                                                                     |
| > (YOLO) object detection algorithm, continues the legacy of its      |
| >                                                                     |
| > predecessors in revolutionizing real-time computer vision           |
| >                                                                     |
| > tasks. Known for its efficiency and speed, YOLOv8 employs a         |
| >                                                                     |
| > convolutional neural network architecture to process images         |
| >                                                                     |
| > in a single pass, swiftly predicting bounding box coordinates       |
| >                                                                     |
| > and class probabilities for detected objects. The                   |
| >                                                                     |
| > advancements introduced in YOLOv8 are geared towards                |
| >                                                                     |
| > improving accuracy and speed, making it a versatile solution        |
| >                                                                     |
| > for applications requiring rapid and precise object detection,      |
| >                                                                     |
| > such as in autonomous vehicles, surveillance systems, and           |
| >                                                                     |
| > robotics. Open source in nature, YOLOv8 provides a platform         |
| >                                                                     |
| > for researchers and developers to access and modify its             |
| >                                                                     |
| > codebase, facilitating customization for specific tasks. Its        |
| >                                                                     |
| > class-agnostic nature enables simultaneous detection and            |
| >                                                                     |
| > classification of multiple object classes, and its training         |
| >                                                                     |
| > procedure involves optimizing parameters based on                   |
| >                                                                     |
| > annotated datasets. YOLOv8 exemplifies the ongoing efforts          |
| >                                                                     |
| > to push the boundaries of real-time object detection,               |
| >                                                                     |
| > contributing to the evolving landscape of computer vision.          |
+=======================================================================+
+-----------------------------------------------------------------------+

14

![](vertopal_4083851ead46433c99b30fbd81535567/media/image2.png){width="9.936111111111112in"
height="0.8222222222222222in"}![](vertopal_4083851ead46433c99b30fbd81535567/media/image3.png){width="11.25in"
height="8.4375in"}

> **Methodology** Proposed Techniques
>
> **Regression**
>
> • Ridge algorithm:

+-----------------------------------------------------------------------+
| > Ridge regression is a linear regression algorithm designed to       |
| >                                                                     |
| > address the challenges of multicollinearity and overfitting in      |
| >                                                                     |
| > predictive modeling. In traditional linear regression, where the    |
| > goal                                                                |
| >                                                                     |
| > is to minimize the sum of squared differences between predicted     |
| >                                                                     |
| > and observed values, multicollinearity among features can lead to   |
| >                                                                     |
| > unstable and inflated coefficient estimates. Ridge regression       |
| >                                                                     |
| > introduces a regularization term, proportional to the square of the |
| >                                                                     |
| > coefficients, to the objective function. This regularization term,  |
| >                                                                     |
| > controlled by a hyperparameter called alpha, discourages the model  |
| >                                                                     |
| > from relying too heavily on any particular features and mitigates   |
| > the                                                                 |
| >                                                                     |
| > impact of multicollinearity. By penalizing large coefficient        |
| > values,                                                             |
| >                                                                     |
| > ridge regression encourages a simpler model that generalizes well   |
| >                                                                     |
| > to new, unseen data. The regularization strength, determined by the |
| >                                                                     |
| > alpha parameter, allows users to fine-tune the balance between      |
| >                                                                     |
| > fitting the training data closely and preventing overfitting. Ridge |
| >                                                                     |
| > regression is a valuable tool in scenarios where datasets exhibit   |
| >                                                                     |
| > correlated features, offering a robust solution for linear          |
| > regression                                                          |
| >                                                                     |
| > tasks in the presence of collinearity.                              |
+=======================================================================+
+-----------------------------------------------------------------------+

15

![](vertopal_4083851ead46433c99b30fbd81535567/media/image2.png){width="9.936111111111112in"
height="0.8222222222222222in"}![](vertopal_4083851ead46433c99b30fbd81535567/media/image3.png){width="11.25in"
height="8.4375in"}

> **Methodology** Proposed Techniques
>
> **Recommendation System**
>
> • Algorithms :

+-----------------------------------------------------------------------+
| > •**Analysis Approach:**                                             |
| >                                                                     |
| > We've used algorithms tailored to analyze solar atlas               |
| >                                                                     |
| > data, particularly focusing on location-based analysis.             |
| >                                                                     |
| > •These include the k-means unsupervised classification              |
| >                                                                     |
| > algorithm. The optimal number of clusters was obtained using        |
| >                                                                     |
| > the elbow method.                                                   |
| >                                                                     |
| > •The latitude and longitude coordinates were converted to           |
| >                                                                     |
| > explicit geographic python objects using Shapely and                |
| >                                                                     |
| > Geopandas. This ensures efficiency and avoids redundancy.           |
| >                                                                     |
| > •**Application in Recommender System:**                             |
| >                                                                     |
| > •The algorithms are likely used to recommend optimal                |
| >                                                                     |
| > locations for solar energy installations, factoring in              |
| >                                                                     |
| > environmental and logistical parameters.                            |
| >                                                                     |
| > •This forms a crucial part of a recommender system aimed at         |
| >                                                                     |
| > aiding decision-making in renewable energy projects.                |
| >                                                                     |
| > •In recommender system, when a new latitude or longitude is         |
| >                                                                     |
| > selected, the location that has similar solar data, is provided.    |
+=======================================================================+
+-----------------------------------------------------------------------+

16

![](vertopal_4083851ead46433c99b30fbd81535567/media/image2.png){width="9.936111111111112in"
height="0.8222222222222222in"}![](vertopal_4083851ead46433c99b30fbd81535567/media/image8.png){width="11.25in"
height="8.614583333333334in"}

> **Discussion and Results**:
>
> For the image classification we have train the mobileNet model for 100
> epochs with 64 batch size, and gained 91% accuracy, and 0.39 loss. See
> below images.

  --------------------------------------------------------------------------------------------------------------------------------------------------------------------
  ![](vertopal_4083851ead46433c99b30fbd81535567/media/image6.png){width="5.625in"   ![](vertopal_4083851ead46433c99b30fbd81535567/media/image7.png){width="5.3125in"
  height="4.625in"}                                                                 height="3.613888888888889in"}
  --------------------------------------------------------------------------------- ----------------------------------------------------------------------------------

  --------------------------------------------------------------------------------------------------------------------------------------------------------------------

> Also. For object detection we trained our data with yolov8, using more
> than 150 epochs, and gained acuracy with 54.1%, precision 52.7%, and
> recall 58.2%.

17

![](vertopal_4083851ead46433c99b30fbd81535567/media/image2.png){width="9.936111111111112in"
height="0.8222222222222222in"}![](vertopal_4083851ead46433c99b30fbd81535567/media/image11.png){width="11.25in"
height="8.4375in"}

> **Discussion and Results**:
>
> In the Ridge Regression model, we trained the model using training
> dataset Split that consist of 54034 records and test it on testing
> dataset split with 14740 records gained 99% R2 Score, 1.08 MSE, 0.67
> MAE, and 1.04 RMAE.
>
> ![](vertopal_4083851ead46433c99b30fbd81535567/media/image9.png){width="10.011111111111111in"
> height="2.2194444444444446in"}
>
> ![](vertopal_4083851ead46433c99b30fbd81535567/media/image10.png){width="9.988888888888889in"
> height="2.2916666666666665in"}
>
> Our recommendation system recommends similar location based on the
> location choose on the map, and provide all required information
> related to each location, such as, latitude, longitude, DNI, DIF, GHI,
> temperature, and PVOUT. In the below figures, in the left figure or
> map, the user choose his preferred location, and the recommended
> locations will be shown on the right figure.

18

![](vertopal_4083851ead46433c99b30fbd81535567/media/image2.png){width="9.936111111111112in"
height="0.8222222222222222in"}![](vertopal_4083851ead46433c99b30fbd81535567/media/image3.png){width="11.25in"
height="8.4375in"}

> **Conclusion and Future Work**

+-----------------------------------------------------------------------+
| In conclusion, this capstone project has presented a comprehensive    |
|                                                                       |
| and innovative approach to enhancing the efficiency and reliability   |
| of                                                                    |
|                                                                       |
| solar panel installations through a combination of deep learning and  |
|                                                                       |
| > traditional machine learning techniques. The utilization of a       |
|                                                                       |
| MobileNet pre-trained model for predicting the failure status of      |
| solar                                                                 |
|                                                                       |
| panels demonstrates the potential of leveraging state-of-the-art      |
|                                                                       |
| image classification methods in the renewable energy sector. The      |
|                                                                       |
| integration of YOLO8 for object detection in solar panels further     |
|                                                                       |
| contributes to the robust monitoring and maintenance of solar         |
|                                                                       |
| installations. Additionally, the application of a Ridge regression    |
|                                                                       |
| model to predict energy generation based on sensor data introduces    |
|                                                                       |
| > a data-driven methodology for optimizing energy output and          |
|                                                                       |
| resource utilization. Lastly, the incorporation of a recommendation   |
|                                                                       |
| system for identifying optimal locations for solar installations in   |
|                                                                       |
| Saudi Arabia underscores the significance of strategic planning for   |
|                                                                       |
| renewable energy deployment. The synergy of these methodologies       |
|                                                                       |
| presents a holistic approach to addressing challenges in the solar    |
|                                                                       |
| > energy domain, promoting sustainability, and fostering              |
|                                                                       |
| advancements in renewable energy technologies and meet our            |
|                                                                       |
| Saudi vision of 2030. This project provides valuable insights that    |
| can                                                                   |
|                                                                       |
| guide future endeavors aimed at enhancing the performance and         |
|                                                                       |
| longevity of solar energy systems, ultimately contributing to a more  |
|                                                                       |
| > sustainable and environmentally conscious energy landscape.         |
+=======================================================================+
+-----------------------------------------------------------------------+

For our future work, we aimed to enhance all of our deep learning,

and machine learning model and use a local Saudi data, then

integrate it along with intelligent robots that automatically detection

> and maintenance, without any human intervention.

19

![](vertopal_4083851ead46433c99b30fbd81535567/media/image2.png){width="9.936111111111112in"
height="0.8222222222222222in"}![](vertopal_4083851ead46433c99b30fbd81535567/media/image3.png){width="11.25in"
height="8.4375in"}

+-----------------------------------------------------------------------+
|   ------------------------------------------------------------------- |
|                                     **Name**                          |
|   --------------------------------- --------------------------------- |
|   **1**                             Dina Bokhamseen                   |
|                                                                       |
|   **2**                             Meshael Almusairii                |
|                                                                       |
|   **3**                             Rewaa Hummedi                     |
|                                                                       |
|   **4**                             Raghad Salem                      |
|   ------------------------------------------------------------------- |
|                                                                       |
| > **Instructor**\                                                     |
| > **Ali H. El-Kassas**                                                |
| >                                                                     |
| > **Teacher Assistant**\                                              |
| > **Rahaf Aladhyani**                                                 |
+=======================================================================+
+-----------------------------------------------------------------------+

> **Team**

20
